---
title: "インシデント対応手順"
label: "インシデント対応手順"
---

## インシデント対応手順

インシデント（障害、セキュリティ侵害など）が発生した際の対応手順を明確に定義することで、迅速かつ適切に対応できます。

### なぜインシデント対応手順が重要なのか

#### 問題のあるインシデント対応

**問題のある状況:**

```
- インシデント発生時に誰に連絡すればいいか分からない
- 対応手順が明確でない
- 情報が散在している
- 同じインシデントが繰り返される
- インシデント後の振り返りが行われない
```

**影響:**
- インシデントの対応に時間がかかる
- ユーザーへの影響が拡大する
- チーム全体のストレスが増える
- 同じ問題が繰り返される

### インシデント対応手順

#### 1. インシデントの定義

**インシデントの種類:**
- **P0（緊急）**: サービスが完全に停止している、または重大なセキュリティ侵害
- **P1（高）**: サービスの主要機能が使用できない
- **P2（中）**: サービスの一部機能が使用できない
- **P3（低）**: 軽微な問題、影響範囲が限定的

**例:**

```
P0（緊急）:
- 本番環境が完全にダウン
- データベースがクラッシュ
- セキュリティ侵害（個人情報漏洩など）

P1（高）:
- 決済機能が使用できない
- ログイン機能が使用できない
- APIのレスポンスが極端に遅い（10秒以上）

P2（中）:
- 一部のページが表示されない
- レポート機能が使用できない
- メール送信が失敗する

P3（低）:
- UIの表示が崩れている
- 軽微なバグ
- パフォーマンスの軽微な劣化
```

#### 2. インシデント対応の流れ

**ステップ1: 検知**

- **監視ツールからのアラート**: CloudWatch、DataDog、Sentryなど
- **ユーザーからの報告**: サポートチケット、Slack、メール
- **チームメンバーからの報告**: 開発中に発見

**ステップ2: トリアージ**

- **優先度の決定**: P0、P1、P2、P3のいずれかに分類
- **影響範囲の確認**: どのユーザー、どの機能に影響があるか
- **対応者の決定**: 誰が対応するか

**ステップ3: 対応**

- **原因の特定**: ログの確認、メトリクスの確認
- **一時的な対応**: ロールバック、サービス停止、トラフィックの制限
- **恒久的な対応**: バグ修正、インフラの修正

**ステップ4: 復旧確認**

- **動作確認**: サービスが正常に動作しているか確認
- **監視**: メトリクスが正常に戻っているか確認
- **ユーザーへの通知**: インシデントが解決したことを通知

**ステップ5: 振り返り**

- **インシデントレポートの作成**: 原因、対応、今後の対策を記録
- **振り返り会議**: チーム全体で振り返り
- **改善アクション**: 再発防止のためのアクションを決定

### インシデント対応の実践

#### 1. インシデント対応チャネル

**Slackチャネル:**
- **#incident**: インシデント対応専用チャネル
- **#incident-p0**: P0インシデント専用チャネル
- **#monitoring**: 監視アラート専用チャネル

**通知ルール:**
- **P0**: 全員に通知、電話での呼び出し
- **P1**: オンコール担当者に通知
- **P2**: 該当チームに通知
- **P3**: 該当チームに通知（緊急ではない）

#### 2. オンコール制度

**オンコール担当者の役割:**
- インシデントの初動対応
- エスカレーションの判断
- インシデントレポートの作成

**オンコールのローテーション:**
- 週単位でローテーション
- 2名以上でオンコール（プライマリ、セカンダリ）
- オンコールカレンダーで管理

**オンコールの報酬:**
- オンコール手当の支給
- オンコール後の休暇の提供

#### 3. インシデントレポートのテンプレート

```markdown
# インシデントレポート

## 基本情報
- **インシデントID**: INC-2024-001
- **発生日時**: 2024年3月15日 14:30 JST
- **検知日時**: 2024年3月15日 14:35 JST
- **解決日時**: 2024年3月15日 15:45 JST
- **影響時間**: 1時間15分
- **優先度**: P1（高）
- **対応者**: 田中、佐藤

## 影響範囲
- **影響を受けたサービス**: 決済API
- **影響を受けたユーザー数**: 約500名
- **影響を受けた機能**: 決済処理

## 原因
- **根本原因**: データベース接続プールの枯渇
- **直接原因**: 大量のリクエストによる接続プールの枯渇
- **根本原因の詳細**: 
  - 接続プールサイズが適切に設定されていなかった
  - 接続の適切なクローズが行われていなかった

## 対応内容
1. **14:35**: インシデント検知（監視アラート）
2. **14:40**: 原因の特定（データベース接続プールの枯渇）
3. **14:45**: 一時的な対応（接続プールサイズの増加）
4. **15:00**: サービス復旧確認
5. **15:30**: 恒久的な対応（接続管理の改善）

## 今後の対策
- [ ] 接続プールサイズの適切な設定
- [ ] 接続の適切なクローズの実装
- [ ] 接続プールの監視アラートの設定
- [ ] 負荷テストの実施

## 学び
- 接続プールの監視が不十分だった
- 負荷テストが不足していた
- インシデント対応の手順が明確でなかった
```

### インシデント対応のベストプラクティス

#### 1. 迅速な対応

- **初動対応**: 5分以内に初動対応を開始
- **エスカレーション**: 30分以内に解決しない場合はエスカレーション
- **コミュニケーション**: 定期的に状況を共有

#### 2. 情報の共有

- **ステータスページ**: ユーザー向けのステータスページを更新
- **内部コミュニケーション**: Slack、メールで状況を共有
- **ドキュメント**: インシデントレポートを記録

#### 3. 振り返り

- **振り返り会議**: インシデント解決後1週間以内に実施
- **改善アクション**: 具体的な改善アクションを決定
- **フォローアップ**: 改善アクションの進捗を確認

### まとめ

インシデント対応手順：

- **インシデントの定義**: P0、P1、P2、P3の優先度分類
- **対応の流れ**: 検知→トリアージ→対応→復旧確認→振り返り
- **実践**: インシデント対応チャネル、オンコール制度、インシデントレポート
- **ベストプラクティス**: 迅速な対応、情報の共有、振り返り

適切なインシデント対応手順により、迅速かつ適切にインシデントに対応できます。

